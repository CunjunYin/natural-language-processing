{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee82352e-5553-48e5-a623-611146796ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing as mp\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from argparse import Namespace\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c6041-e4e1-4567-ae7a-20fb9dac9b5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3746352-af85-4ef2-81c9-2e696aecf858",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not nltk.find('corpora/wordnet'):\n",
    "     nltk.download('wordnet')\n",
    "porter_stemmer  = PorterStemmer()\n",
    "lemmatizer      = WordNetLemmatizer()\n",
    "regex_tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "spell  = SpellChecker()\n",
    "one_hot_vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    degree_injury_file=\"./data/degreeinjury.csv\",\n",
    "    injury_bodyparts_file=\"./data/injurybodyparts.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.h5\",\n",
    "    save_dir=\"model_storage/document_classification\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='./Glove/glove.6B.200d.txt', \n",
    "    use_glove=True,\n",
    "    embedding_size=150, \n",
    "    # Training hyper parameter\n",
    "    window_size=5,\n",
    "    val_proportion=0.1,\n",
    "    test_proportion=0.2,\n",
    "    learning_rate = 0.001,\n",
    "    seed=666,\n",
    "    dropout_p=0.1, \n",
    "    batch_size=256, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd08af5-d4b0-416b-a9c8-8e7306df0125",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373dae4e-bd9c-4c88-bded-888e4eb1a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings, embedding_size\n",
    "\n",
    "def load_glove_from_file(glove_filepath):\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, encoding=\"utf8\") as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "def clean_text(text):\n",
    "    txt = ''\n",
    "    txt = txt.strip()\n",
    "    txt = re.sub(r\"([.,!?])\", r\" \\1 \", txt)\n",
    "    for word in tokenize(text):\n",
    "        # The EE is short hand on employee\n",
    "        if word=='ee' or word == 'EE':\n",
    "            txt += ' employee '\n",
    "        elif word in string.punctuation:\n",
    "            txt = txt + ' ' +word\n",
    "        elif len(word)>2:\n",
    "            word = spell.correction(word)\n",
    "            txt = txt + ' ' + word.lower().strip()\n",
    "    txt = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", txt)\n",
    "    return txt.strip()\n",
    "\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(words, df=False):\n",
    "    stemmed_words=[porter_stemmer.stem(word) for word in words]\n",
    "    if df:\n",
    "        return pd.DataFrame({'original': words,'stemmed': stemmed_words})\n",
    "    return stemmed_words\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize(words, df=False):\n",
    "    lemmatized_words=[]\n",
    "    tagged_sent = nltk.pos_tag(words)\n",
    "    \n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or nltk.corpus.wordnet.NOUN\n",
    "        if wordnet_pos is None:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(tag[0]))\n",
    "        else:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(tag[0], pos=wordnet_pos))\n",
    "    \n",
    "    if df:\n",
    "        return pd.DataFrame({'original': words,'lemmatized': lemmatized_words})\n",
    "    return lemmatized_words\n",
    "\n",
    "def SK_TFIDF_stopwords(corpus, vectorizer):\n",
    "    vectorizer.fit(corpus)\n",
    "    X = vectorizer.transform(corpus)\n",
    "    return vectorizer.stop_words_, X\n",
    "\n",
    "def get_words(texts):\n",
    "    words = []\n",
    "    for text in texts:\n",
    "        words+=regex_tokenizer.tokenize(text)\n",
    "    return np.asarray(words).reshape(-1, 1)\n",
    "\n",
    "def OHE_2D(sent, sparse=False):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse=sparse)\n",
    "    words=[]\n",
    "    for w in sent.values:\n",
    "        words += w\n",
    "    X = enc.fit(np.array(words).reshape(-1,1))\n",
    "    \n",
    "    # Round up to without unit digit\n",
    "    ohe = sent.apply(lambda x: enc.transform(np.array(x).reshape(-1,1)))\n",
    "    maxwords = max([len(x) for x in ohe])\n",
    "    maxwords = math.ceil(maxwords/10)*10\n",
    "    dim = len(ohe[0][0])\n",
    "    print('Vector length:          ', dim, '\\nMaximum number of words:', maxwords)\n",
    "\n",
    "    return [np.concatenate((x, np.zeros((maxwords - len(x), dim))), axis=0) for x in ohe]\n",
    "\n",
    "def OHE_1D(sent):\n",
    "    corpus = [' '.join(x) for x in sent.values]\n",
    "    print(corpus[0])\n",
    "    ohe = CountVectorizer(binary=True)\n",
    "    return pd.DataFrame(ohe.fit_transform(corpus).todense(), columns=ohe.get_feature_names())\n",
    "\n",
    "def spell_check(words):\n",
    "    return [spell.correction(word) for word in words]\n",
    "\n",
    "def remove_stopwords(words, stopwords):\n",
    "    return [w for w in words if w not in stopwords]\n",
    "\n",
    "def split_save(data, path, test=0.2, valid= 0.1):\n",
    "    train, test  = train_test_split(data,  test_size=test, random_state=666, shuffle=True, stratify=data['target'])\n",
    "    train, valid = train_test_split(train, test_size=valid, random_state=666, shuffle=True, stratify=train['target'])\n",
    "    train['split'] = 'train'\n",
    "    test['split']  = 'test'\n",
    "    valid['split'] = 'val'\n",
    "    pd.concat([train, valid, test]).to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4dde7-dae6-4523-b000-93ba6186655d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clean Narrative\n",
    "#### Clean Narrative is computionally expensive run if only necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b9e2af-9940-49c7-92c3-515d1cb781cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEGREE_INJURY</th>\n",
       "      <th>DEGREE_INJURY_CD</th>\n",
       "      <th>INJ_BODY_PART_CD</th>\n",
       "      <th>INJ_BODY_PART</th>\n",
       "      <th>NARRATIVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DAYS RESTRICTED ACTIVITY ONLY</td>\n",
       "      <td>5</td>\n",
       "      <td>700</td>\n",
       "      <td>MULTIPLE PARTS (MORE THAN ONE MAJOR)</td>\n",
       "      <td>Employee was cleaning up at the Primary Crushe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   DEGREE_INJURY DEGREE_INJURY_CD INJ_BODY_PART_CD  \\\n",
       "0  DAYS RESTRICTED ACTIVITY ONLY                5              700   \n",
       "\n",
       "                          INJ_BODY_PART  \\\n",
       "0  MULTIPLE PARTS (MORE THAN ONE MAJOR)   \n",
       "\n",
       "                                           NARRATIVE  \n",
       "0  Employee was cleaning up at the Primary Crushe...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('us_data_2000.csv')\n",
    "df = df[['DEGREE_INJURY', 'DEGREE_INJURY_CD', 'INJ_BODY_PART_CD', 'INJ_BODY_PART', 'NARRATIVE']]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa999f8c-7ac3-4509-9ba1-e48b37b2f51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEGREE_INJURY       False\n",
       "DEGREE_INJURY_CD    False\n",
       "INJ_BODY_PART_CD    False\n",
       "INJ_BODY_PART       False\n",
       "NARRATIVE           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a409ba55-0332-4b0c-b284-f53034bad9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NARRATIVE NaN count:    0\n",
      "NARRATIVE len>10 count: 1\n",
      "\n",
      "DEGREE_INJURY_CD \"?\" count:  11\n",
      "\n",
      "INJ_BODY_PART_CD \"?\" count:  242\n"
     ]
    }
   ],
   "source": [
    "print('NARRATIVE NaN count:   ', df[df['NARRATIVE'].isna()].shape[0])\n",
    "print('NARRATIVE len>10 count:', df[df['NARRATIVE'].str.len() < 10].shape[0])\n",
    "print('\\nDEGREE_INJURY_CD \"?\" count: ', df[df['DEGREE_INJURY_CD'] == '?'].shape[0])\n",
    "print('\\nINJ_BODY_PART_CD \"?\" count: ', df[df['INJ_BODY_PART_CD'] == '?'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344ae3a-1e8c-4ac7-a56a-5f327fb664f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Remove Narrative are too short and NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1531e8db-f9c6-434c-afb2-754c279b0b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin data size:       2000\n",
      "Clean data size: 1999\n"
     ]
    }
   ],
   "source": [
    "print(\"Origin data size:      \", df.shape[0])\n",
    "df = df[df['NARRATIVE'].notna()]\n",
    "df = df[df['NARRATIVE'].str.len() > 10]\n",
    "print(\"Clean data size:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc34fe-6333-4ef7-b23d-56376feafb19",
   "metadata": {
    "tags": []
   },
   "source": [
    "### To Lower case and correct words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72accc8a-fe28-4502-bcd1-ea266391ad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'employee was cleaning the primary crusher with the dingo skid steer . the employee slipped and fell while operating the skid steer and the machine pinned him against the cement retaining wall .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    df['NARRATIVE'] = pool.map(clean_text, df['NARRATIVE'])\n",
    "df['NARRATIVE'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee25e3-3289-45af-8925-fc9fb978cc4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset for task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79ee30f7-43ad-4c41-b34e-9bfefdcdf054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEGREE_INJURY_CD</th>\n",
       "      <th>DEGREE_INJURY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ACCIDENT ONLY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FATALITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PERM TOT OR PERM PRTL DISABLTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DAYS AWAY FROM WORK ONLY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>DYS AWY FRM WRK &amp; RESTRCTD ACT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>DAYS RESTRICTED ACTIVITY ONLY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>NO DYS AWY FRM WRK,NO RSTR ACT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>OCCUPATNAL ILLNESS NOT DEG 1-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>INJURIES DUE TO NATURAL CAUSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>INJURIES INVOLVNG NONEMPLOYEES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>ALL OTHER CASES (INCL 1ST AID)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DEGREE_INJURY_CD                   DEGREE_INJURY\n",
       "0                  0                   ACCIDENT ONLY\n",
       "1                  1                        FATALITY\n",
       "2                  2  PERM TOT OR PERM PRTL DISABLTY\n",
       "3                  3        DAYS AWAY FROM WORK ONLY\n",
       "4                  4  DYS AWY FRM WRK & RESTRCTD ACT\n",
       "5                  5   DAYS RESTRICTED ACTIVITY ONLY\n",
       "6                  6  NO DYS AWY FRM WRK,NO RSTR ACT\n",
       "7                  7  OCCUPATNAL ILLNESS NOT DEG 1-6\n",
       "8                  8  INJURIES DUE TO NATURAL CAUSES\n",
       "9                  9  INJURIES INVOLVNG NONEMPLOYEES\n",
       "10                10  ALL OTHER CASES (INCL 1ST AID)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_degree=df[['NARRATIVE', 'DEGREE_INJURY_CD', 'DEGREE_INJURY']].copy()\n",
    "df_degree = df_degree[df_degree['DEGREE_INJURY_CD'] != '?']\n",
    "df_degree['DEGREE_INJURY_CD'] = pd.to_numeric(df_degree['DEGREE_INJURY_CD'])\n",
    "\n",
    "def get_degree_injury(dataframe):\n",
    "    CD = []\n",
    "    INJURY = []\n",
    "    for i in range(0, len(dataframe.DEGREE_INJURY_CD.unique())):\n",
    "        CD.append(i)\n",
    "        INJURY.append(dataframe[dataframe['DEGREE_INJURY_CD'] == i]['DEGREE_INJURY'].values[0])\n",
    "    return pd.DataFrame({'DEGREE_INJURY_CD': CD,'DEGREE_INJURY': INJURY})\n",
    "\n",
    "get_degree_injury(df_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19dd8f6c-422e-4b74-91a4-ccb53a12e201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAYS AWAY FROM WORK ONLY          29.879276\n",
       "NO DYS AWY FRM WRK,NO RSTR ACT    27.766600\n",
       "DAYS RESTRICTED ACTIVITY ONLY     18.058350\n",
       "ACCIDENT ONLY                     11.016097\n",
       "DYS AWY FRM WRK & RESTRCTD ACT     7.293763\n",
       "OCCUPATNAL ILLNESS NOT DEG 1-6     2.867203\n",
       "ALL OTHER CASES (INCL 1ST AID)     1.006036\n",
       "PERM TOT OR PERM PRTL DISABLTY     0.905433\n",
       "FATALITY                           0.553320\n",
       "INJURIES DUE TO NATURAL CAUSES     0.503018\n",
       "INJURIES INVOLVNG NONEMPLOYEES     0.150905\n",
       "Name: DEGREE_INJURY, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*df_degree['DEGREE_INJURY'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9053eed8-431c-44e9-aa96-69dd42f575fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = [\n",
    "    'DAYS AWAY FROM WORK ONLY',\n",
    "    'DAYS RESTRICTED ACTIVITY ONLY',\n",
    "    'DYS AWY FRM WRK & RESTRCTD ACT'\n",
    "]\n",
    "df_degree['target'] = df_degree['DEGREE_INJURY'].isin(group1)\n",
    "df_degree[\"target\"] = df_degree[\"target\"].astype(str)\n",
    "del df_degree['DEGREE_INJURY']\n",
    "del df_degree['DEGREE_INJURY_CD']\n",
    "split_save(df_degree, path=args.degree_injury_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c74cf-db27-4aee-8057-8171206b8e5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9296b6ce-4dbd-42ca-8eb9-0bcc92d1b364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes {'Multiple Parts', 'Leg', 'Head', 'Arm', 'Body'}\n",
      "Instances 1755\n"
     ]
    }
   ],
   "source": [
    "HEAD     = ['SKULL', 'EAR(S) INTERNAL & HEARING', 'EAR(S) EXTERNAL', 'EAR(S) INTERNAL & EXTERNAL' ,'EYE(S) OPTIC NERVE/VISON', 'NOSE/NASAL PASSAGES/SINUS/SMELL', 'EAR(S) INTERNAL & HEARING', 'BRAIN', 'FACE,NEC', 'FACE, MULTIPLE PARTS', 'HEAD, MULTIPLE PARTS', 'HEAD,NEC', 'MULTIPLE PARTS', 'MOUTH/LIP/TEETH/TONGUE/THROAT/TASTE', 'JAW INCLUDE CHIN', 'SCALP', 'NECK']\n",
    "LEG      = ['LEG, MULTIPLE PARTS', 'LOWER LEG/TIBIA/FIBULA', 'FOOT(NOT ANKLE/TOE)/TARSUS/METATARSUS', 'LEG, NEC', 'KNEE/PATELLA', 'TOE(S)/PHALANGES', 'ANKLE', 'THIGH/FEMUR', 'LOWER EXTREMITIES,NEC', 'LOWER EXTREMITIES, MULTIPLE PARTS']\n",
    "ARM      = ['ARM, MULTIPLE PARTS', 'ARM,NEC', 'HAND (NOT WRIST OR FINGERS)', 'WRIST', 'FINGER(S)/THUMB', 'UPPER ARM/HUMERUS', 'SHOULDERS (COLLARBONE/CLAVICLE/SCAPULA)', 'ELBOW', 'FOREARM/ULNAR/RADIUS', 'UPPER EXTREMITIES, MULTIPLE', 'UPPER EXTREMITIES, NEC']\n",
    "BODY     = ['CHEST (RIBS/BREAST BONE/CHEST ORGNS)', 'BODY SYSTEMS', 'ABDOMEN/INTERNAL ORGANS', 'BODY PARTS, NEC', 'TRUNK, MULTIPLE PARTS', 'TRUNK,NEC', 'HIPS (PELVIS/ORGANS/KIDNEYS/BUTTOCKS)', 'BACK (MUSCLES/SPINE/S-CORD/TAILBONE)']\n",
    "MULTIPLE = ['MULTIPLE PARTS (MORE THAN ONE MAJOR)']\n",
    "\n",
    "df_parts=df[['NARRATIVE', 'INJ_BODY_PART_CD', 'INJ_BODY_PART']].copy()\n",
    "df_parts = df_parts[df_parts['INJ_BODY_PART_CD'] != '?']\n",
    "del df_parts['INJ_BODY_PART_CD']\n",
    "\n",
    "df_parts['INJ_BODY_PART'][df_parts['INJ_BODY_PART'].isin(HEAD)]     = 'Head'\n",
    "df_parts['INJ_BODY_PART'][df_parts['INJ_BODY_PART'].isin(LEG)]      = 'Leg'\n",
    "df_parts['INJ_BODY_PART'][df_parts['INJ_BODY_PART'].isin(ARM)]      = 'Arm'\n",
    "df_parts['INJ_BODY_PART'][df_parts['INJ_BODY_PART'].isin(BODY)]     = 'Body'\n",
    "df_parts['INJ_BODY_PART'][df_parts['INJ_BODY_PART'].isin(MULTIPLE)] = 'Multiple Parts'\n",
    "df_parts['target'] = df_parts['INJ_BODY_PART']\n",
    "df_parts = df_parts[df_parts['target']!='UNCLASSIFIED']\n",
    "del df_parts['INJ_BODY_PART']\n",
    "split_save(df_parts, path=args.injury_bodyparts_file)\n",
    "print(\"classes\", set(df_parts.target))\n",
    "print(\"Instances\", df_parts.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb361bb-cfd7-46eb-8812-265b347bed1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset for task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4ca632e-961a-474c-909d-2cc63bcd7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e81ab-b43f-4682-9ad6-a2e612bb7877",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Prepreocessing and Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e6ae0e-c8ae-475f-bbd0-8f2ff1253ce9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test lem and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32b354e0-d4d8-4957-ac9a-f84d078a8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem      = stem(nltk.tokenize.word_tokenize((df['NARRATIVE'].values[0])), df=True)\n",
    "df_lemmatize = lemmatize(nltk.tokenize.word_tokenize((df['NARRATIVE'].values[0])), df=True)\n",
    "df_stem['lemmatized'] = df_lemmatize['lemmatized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae202934-1156-47ff-b8c6-0f6e04a3a9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>employee</td>\n",
       "      <td>employe</td>\n",
       "      <td>employee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>wa</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>primary</td>\n",
       "      <td>primari</td>\n",
       "      <td>primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>employee</td>\n",
       "      <td>employe</td>\n",
       "      <td>employee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>operating</td>\n",
       "      <td>oper</td>\n",
       "      <td>operate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>machine</td>\n",
       "      <td>machin</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     original  stemmed lemmatized\n",
       "0    employee  employe   employee\n",
       "1         was       wa         be\n",
       "4     primary  primari    primary\n",
       "13   employee  employe   employee\n",
       "18  operating     oper    operate\n",
       "24    machine   machin    machine"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stem[df_stem['stemmed'] != df_stem['lemmatized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a263af8-78cf-464c-a1d8-7da557d0f946",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7c82ddc-184e-4ae9-bbf7-d0656d73ce14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 27\n",
      "Number of terms    : 3511\n",
      "Ascending\n",
      "               term      rank\n",
      "2150  perpendicular  0.142870\n",
      "943       elevation  0.142870\n",
      "225            baby  0.148709\n",
      "3035       swelling  0.148709\n",
      "39      acupuncture  0.148709\n",
      "3403             wd  0.152215\n",
      "3295    unknowingly  0.152215\n",
      "3183            tow  0.158576\n",
      "144      applicator  0.158644\n",
      "2737        silicon  0.158644\n",
      "2115    participate  0.158733\n",
      "1017       exercise  0.158733\n",
      "3157             to  0.162485\n",
      "1469            ind  0.162485\n",
      "2453     relocating  0.167219\n",
      "\n",
      "Descending\n",
      "       term       rank\n",
      "2550   rock  43.301234\n",
      "343    bolt  42.370124\n",
      "2917   step  36.435148\n",
      "2173  piece  34.772444\n",
      "1152   foot  34.292901\n",
      "2096   pain  33.907957\n",
      "3231  truck  32.252625\n",
      "1080   felt  31.555692\n",
      "1376    hit  30.986592\n",
      "289    belt  30.728900\n",
      "1614   knee  29.765978\n",
      "859    down  29.416297\n",
      "1235    get  29.267398\n",
      "1369    him  29.053929\n",
      "1535   into  28.936520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cun/anaconda3/envs/CITS4012/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.1, min_df=1)\n",
    "_df = df[[\"NARRATIVE\"]].copy()\n",
    "_df['NARRATIVE'] = _df['NARRATIVE'].apply(lambda x: tokenize(x))\n",
    "_df['NARRATIVE'] = _df['NARRATIVE'].apply(lambda x: lemmatize(x))\n",
    "# _df['NARRATIVE'] = _df['NARRATIVE'].apply(lambda x: stem(x))\n",
    "\n",
    "stopwrods, X = SK_TFIDF_stopwords(_df['NARRATIVE'].apply(lambda x: ' '.join(x)), vectorizer)\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# sum tfidf frequency of each term through documents\n",
    "sums = X.sum(axis=0)\n",
    "\n",
    "# connecting term to its sums frequency\n",
    "data = []\n",
    "for col, term in enumerate(terms):\n",
    "    data.append( (term, sums[0,col] ))\n",
    "print('Number of stopwords:', len(stopwrods))\n",
    "print('Number of terms    :', len(terms))\n",
    "\n",
    "ranking = pd.DataFrame(data, columns=['term','rank'])\n",
    "print('Ascending')\n",
    "print(ranking.sort_values('rank', ascending=True)[0:15])\n",
    "\n",
    "print('\\nDescending')\n",
    "print(ranking.sort_values('rank', ascending=False)[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d03ca7d-7746-4856-8b84-5e4b851bbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add words that could be meaningless\n",
    "for w in ['abb','abc','dia', 'yes','yee', 'that', 'ind', 'wd', 'to', 'him']:\n",
    "        stopwrods.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f21a3513-e07f-4ee0-a450-9db0e40ea989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'back', 'out', 'when', 'left', 'abb', 'that', 'roof', 'fell', 'his', 'off', 'ind', 'cause', 'with', 'work', 'not', 'right', 'have', 'yee', 'be', 'finger', 'him', 'yes', 'from', 'cut', 'hand', 'abc', 'the', 'slip', 'fall', 'strike', 'dia', 'wd', 'to', 'while', 'and', 'employee'}\n"
     ]
    }
   ],
   "source": [
    "print(stopwrods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780bb66-4bb3-467e-931d-f5aeace0248a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Function repeative Prepreocessing and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a8028de-ca99-45a2-b3d2-b2dada480db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_lem_sem(text):\n",
    "    words = tokenize(text)\n",
    "    words = lemmatize(words)\n",
    "#     words = stem(words)\n",
    "    return words\n",
    "\n",
    "def removestop_join(words, stopwords):\n",
    "    words = remove_stopwords(words, stopwords)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def prepreocessing(data, stop, target=\"NARRATIVE\"):\n",
    "    _df=data.copy()\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        _df[target] = pool.map(tok_lem_sem, _df[target])\n",
    "    _df[target] = _df.apply(lambda row: removestop_join(row[target], stop), axis=1)\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d510aee-36c4-4431-a494-55886e604e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepreocessing(pd.read_csv(args.degree_injury_file), stopwrods).to_csv(args.degree_injury_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84e1d04a-04bd-46d5-b006-be354fafe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepreocessing(pd.read_csv(args.injury_bodyparts_file), stopwrods).to_csv(args.injury_bodyparts_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc13934-5ed9-49aa-ae8f-1784a91506a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Binary Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa76e91-1ab8-4277-80a4-d24a3919821a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## One Hoe Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e37347-1b5d-4d94-a2d7-6555776adb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OHEVocabulary(object):\n",
    "    \"\"\"Class to process text and extract Vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                                for token, idx in self._token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = 1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx, 'add_unk': self._add_unk, 'unk_token': self._unk_token}\n",
    "    @classmethod\n",
    "    \n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "class OHESequenceVocabulary(OHEVocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(OHESequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(OHESequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "class OHEVectorizer(object):\n",
    "    def __init__(self, NARRATIVE_vocab, target_vocab):\n",
    "        self.NARRATIVE_vocab = NARRATIVE_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def vectorize(self, NARRATIVE):\n",
    "        one_hot = np.zeros(len(self.NARRATIVE_vocab), dtype=np.float32)\n",
    "        for token in NARRATIVE.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.NARRATIVE_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cutoff=15):\n",
    "        target_vocab = OHEVocabulary()        \n",
    "        for target in sorted(set(df.target)):\n",
    "            target_vocab.add_token(target)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for NARRATIVE in df.NARRATIVE:\n",
    "            for token in NARRATIVE.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "        \n",
    "        NARRATIVE_vocab = OHESequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                NARRATIVE_vocab.add_token(word)\n",
    "        \n",
    "        return cls(NARRATIVE_vocab, target_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        NARRATIVE_vocab =   OHESequenceVocabulary.from_serializable(contents['NARRATIVE_vocab'])\n",
    "        target_vocab =  OHEVocabulary.from_serializable(contents['target_vocab'])\n",
    "\n",
    "        return cls(NARRATIVE_vocab=NARRATIVE_vocab, target_vocab=target_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'NARRATIVE_vocab': self.NARRATIVE_vocab.to_serializable(),\n",
    "                'target_vocab': self.target_vocab.to_serializable()}\n",
    "\n",
    "class OHEDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, df.NARRATIVE)) + 2\n",
    "        \n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),'val': (self.val_df, self.validation_size),'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, csv):\n",
    "        df = pd.read_csv(csv)\n",
    "        return cls(df, OHEVectorizer.from_dataframe(df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n",
    "        df = pd.read_csv(news_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(news_csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        NARRATIVE_vector = self._vectorizer.vectorize(row.NARRATIVE)\n",
    "        target_index = self._vectorizer.target_vocab.lookup_token(row.target)-1\n",
    "        return {'x_data': NARRATIVE_vector,'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "class OHEClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim=1):\n",
    "        super(OHEClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim) \n",
    "    \n",
    "    def forward(self, x, apply_sigmoid=False):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        out = self.fc4(out)\n",
    "        \n",
    "        if apply_sigmoid:\n",
    "            out = torch.sigmoid(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02dd841-eced-418f-9d49-20606a00fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = make_train_state(args)\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# dataset and vectorizer\n",
    "dataset = OHEDataset.load_dataset_and_make_vectorizer(args.degree_injury_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "# model\n",
    "classifier = OHEClassifier(input_dim=len(vectorizer.NARRATIVE_vocab), hidden_dims=[256, 256, 128])\n",
    "classifier = classifier.to(args.device)\n",
    "# loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.0001)\n",
    "train_state = make_train_state(args)\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6540ca4-c914-4033-8530-d2398db70d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.NARRATIVE_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854b2a14-15c1-4ec0-aa21-b616b781d048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6902 Train acc 55.2061 Val loss 0.6897 Val acc 55.3459\n",
      "Train loss 0.6888 Train acc 55.2061 Val loss 0.6885 Val acc 55.3459\n",
      "Train loss 0.6874 Train acc 55.2061 Val loss 0.6873 Val acc 55.3459\n",
      "Train loss 0.6858 Train acc 55.2061 Val loss 0.6861 Val acc 55.3459\n",
      "Train loss 0.6837 Train acc 55.2061 Val loss 0.6845 Val acc 55.3459\n",
      "Train loss 0.6811 Train acc 55.276 Val loss 0.6825 Val acc 55.3459\n",
      "Train loss 0.6775 Train acc 55.7652 Val loss 0.68 Val acc 55.3459\n",
      "Train loss 0.6727 Train acc 57.2327 Val loss 0.6767 Val acc 57.8616\n",
      "Train loss 0.6663 Train acc 59.399 Val loss 0.6723 Val acc 57.8616\n",
      "Train loss 0.6579 Train acc 60.9364 Val loss 0.6667 Val acc 59.1195\n",
      "Train loss 0.6471 Train acc 63.8714 Val loss 0.6595 Val acc 62.2642\n",
      "Train loss 0.6334 Train acc 68.4137 Val loss 0.6507 Val acc 61.0063\n",
      "Train loss 0.6164 Train acc 71.6981 Val loss 0.64 Val acc 61.0063\n",
      "Train loss 0.596 Train acc 76.1705 Val loss 0.6274 Val acc 62.8931\n",
      "Train loss 0.572 Train acc 78.5465 Val loss 0.6129 Val acc 66.0377\n",
      "Train loss 0.5446 Train acc 80.7128 Val loss 0.597 Val acc 70.4403\n",
      "Train loss 0.5143 Train acc 82.1104 Val loss 0.5804 Val acc 72.327\n",
      "Train loss 0.4821 Train acc 83.2984 Val loss 0.5642 Val acc 74.2138\n",
      "Train loss 0.449 Train acc 85.1852 Val loss 0.5492 Val acc 76.1006\n",
      "Train loss 0.4161 Train acc 86.5129 Val loss 0.5364 Val acc 75.4717\n",
      "Train loss 0.3844 Train acc 87.5611 Val loss 0.5264 Val acc 77.3585\n",
      "Train loss 0.3543 Train acc 88.3997 Val loss 0.5196 Val acc 77.3585\n",
      "Train loss 0.3262 Train acc 89.7275 Val loss 0.516 Val acc 76.7296\n",
      "Train loss 0.3001 Train acc 91.4745 Val loss 0.5156 Val acc 75.4717\n",
      "Train loss 0.2757 Train acc 92.3829 Val loss 0.518 Val acc 74.8428\n",
      "Train loss 0.2531 Train acc 93.0119 Val loss 0.5226 Val acc 74.8428\n",
      "Train loss 0.232 Train acc 94.2697 Val loss 0.5289 Val acc 74.8428\n",
      "Train loss 0.2123 Train acc 94.8288 Val loss 0.5368 Val acc 76.1006\n",
      "Train loss 0.1938 Train acc 95.4577 Val loss 0.546 Val acc 76.1006\n",
      "Train loss 0.1765 Train acc 96.0168 Val loss 0.5561 Val acc 76.1006\n"
     ]
    }
   ],
   "source": [
    "best_val  = {\n",
    "    'epoch': 0,\n",
    "    'model_state_dict': 0,\n",
    "    'optimizer_state_dict': 0,\n",
    "    'loss': 0,\n",
    "    'acc':0\n",
    "}\n",
    "for epoch_index in range(150):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    # Iterate over training dataset\n",
    "    # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size=len(dataset), device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # the training routine is 5 steps:\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # step 2. compute the output\n",
    "        y_pred = classifier(batch_dict['x_data'].float())\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch-running_loss) / (batch_index + 1)\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, batch_size=len(dataset), device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # step 1. compute the output\n",
    "        y_pred = classifier(batch_dict['x_data'].float())\n",
    "        # step 2. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        # step 3. compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    if(best_val['acc'] < running_acc):\n",
    "        best_val  = {\n",
    "            'epoch': epoch_index,\n",
    "            'model_state_dict': classifier.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss,\n",
    "            'acc' : running_acc\n",
    "        }\n",
    "    if epoch_index%5 == 0:\n",
    "        print('Train loss', round(train_state['train_loss'][-1], 4), 'Train acc', round(train_state['train_acc'][-1], 4), 'Val loss', round(train_state['val_loss'][-1], 4), 'Val acc', round(train_state['val_acc'][-1], 4))\n",
    "torch.save(best_val, \"OHE.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e426c5-f7ac-42b4-81b9-1043a8b04d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=len(dataset),device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "checkpoint = torch.load(\"OHE.h5\")\n",
    "classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "classifier.eval()\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x=batch_dict['x_data'].float())\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "    # compute the accuracy\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a58987a1-242b-4099-8ca0-bbeec58e7d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.662\n",
      "Test Accuracy: 73.12\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdeb75-5545-4b4a-966b-d78673ef583d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fd16e6d-5631-4084-9c0e-07e5029b2fd6",
   "metadata": {},
   "source": [
    "## Word embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74fabab9-6b8e-4a62-b2fb-c1027b75e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    \n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "class GloveVectorizer(object):  \n",
    "    def __init__(self, NARRATIVE_vocab, target_vocab):\n",
    "        self.NARRATIVE_vocab = NARRATIVE_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def vectorize(self, NARRATIVE, vector_length=-1):\n",
    "        indices = [self.NARRATIVE_vocab.begin_seq_index]\n",
    "        indices.extend(self.NARRATIVE_vocab.lookup_token(token) for token in NARRATIVE.split(\" \") if token not in string.punctuation)\n",
    "        indices.append(self.NARRATIVE_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.NARRATIVE_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cutoff=5):\n",
    "        target_vocab = Vocabulary()        \n",
    "        for target in sorted(set(df.target)):\n",
    "            target_vocab.add_token(target)\n",
    "        \n",
    "        word_counts = Counter()\n",
    "        for NARRATIVE in df.NARRATIVE:\n",
    "            for token in NARRATIVE.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "        \n",
    "        NARRATIVE_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                NARRATIVE_vocab.add_token(word)\n",
    "#             else: \n",
    "#                 print(word)\n",
    "        \n",
    "        return cls(NARRATIVE_vocab, target_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_vocab = SequenceVocabulary.from_serializable(contents['NARRATIVE_vocab'])\n",
    "        category_vocab = Vocabulary.from_serializable(contents['target_vocab'])\n",
    "        return cls(NARRATIVE_vocab=NARRATIVE_vocab, target_vocab=target_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'NARRATIVE_vocab': self.NARRATIVE_vocab.to_serializable(), 'target_vocab': self.target_vocab.to_serializable()}\n",
    "\n",
    "class GloveDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, df.NARRATIVE)) + 2\n",
    "        \n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size), 'val': (self.val_df, self.validation_size), 'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        class_counts = df.target.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.target_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, news_csv):\n",
    "        df = pd.read_csv(news_csv)\n",
    "        train_df = df[df.split=='train']\n",
    "        return cls(df, GloveVectorizer.from_dataframe(train_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n",
    "        df = pd.read_csv(news_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(news_csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        NARRATIVE_vector = self._vectorizer.vectorize(row.NARRATIVE, self._max_seq_length)\n",
    "        target_index = self._vectorizer.target_vocab.lookup_token(row.target) #-1\n",
    "        return {'x_data': NARRATIVE_vector,'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "class GloveClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_channels, hidden_dim, num_classes, dropout_p, pretrained_embeddings=None, padding_idx=0):\n",
    "        super(GloveClassifier, self).__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size, num_embeddings=num_embeddings, padding_idx=padding_idx)        \n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size, num_embeddings=num_embeddings, padding_idx=padding_idx,      _weight=pretrained_embeddings)\n",
    "            \n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_size, out_channels=num_channels[0], kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=num_channels[0], out_channels=num_channels[1], kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=num_channels[1], out_channels=num_channels[2], kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=num_channels[2], out_channels=num_channels[3], kernel_size=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        #MLP         \n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels[3], hidden_dim[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim[1], num_classes)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        # embed and permute so features are channels\n",
    "        x_embedded = self.emb(x_in).permute(0, 2, 1)\n",
    "        features = self.convnet(x_embedded)\n",
    "        # average and remove the extra dimension\n",
    "        remaining_size = features.size(dim=2)\n",
    "        features = nn.functional.max_pool1d(features, remaining_size).squeeze(dim=2)\n",
    "        features = nn.functional.dropout(features, p=self._dropout_p)\n",
    "        \n",
    "        out = self.fc1(features)\n",
    "#         out = nn.functional.dropout(out, p=self._dropout_p)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "\n",
    "        if apply_softmax:\n",
    "            out = torch.nn.functional.softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7197d34-4ece-47a0-92c7-c70aa5a363b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = GloveDataset.load_dataset_and_load_vectorizer(args.degree_injury_file, args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = GloveDataset.load_dataset_and_make_vectorizer(args.degree_injury_file)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.NARRATIVE_vocab._token_to_idx.keys()\n",
    "    embeddings, embedding_size = make_embedding_matrix(glove_filepath=args.glove_filepath, words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ab632fa-7fc4-4652-a6fb-4ee16b1beafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GloveClassifier(\n",
    "    embedding_size=embedding_size, \n",
    "    num_embeddings=len(vectorizer.NARRATIVE_vocab),\n",
    "    num_channels=[128, 64, 32, 32],\n",
    "    hidden_dim=[32, 16], \n",
    "    num_classes=len(vectorizer.target_vocab), \n",
    "    dropout_p=0.3,\n",
    "    pretrained_embeddings=embeddings,\n",
    "    padding_idx=0\n",
    ")\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9526e632-507c-43c7-99a8-289fc824d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cun/anaconda3/envs/CITS4012/lib/python3.8/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6942 Train acc 44.7939 Val loss 0.6945 Val acc 44.6541\n",
      "Train loss 0.6939 Train acc 44.7939 Val loss 0.6943 Val acc 44.6541\n",
      "Train loss 0.6938 Train acc 44.7939 Val loss 0.694 Val acc 44.6541\n",
      "Train loss 0.6937 Train acc 44.7939 Val loss 0.6939 Val acc 44.6541\n",
      "Train loss 0.6934 Train acc 44.7939 Val loss 0.6937 Val acc 44.6541\n",
      "Train loss 0.6928 Train acc 44.7939 Val loss 0.6931 Val acc 44.6541\n",
      "Train loss 0.6917 Train acc 44.7939 Val loss 0.6927 Val acc 44.6541\n",
      "Train loss 0.6894 Train acc 44.7939 Val loss 0.6906 Val acc 44.6541\n",
      "Train loss 0.6857 Train acc 44.7939 Val loss 0.6888 Val acc 44.6541\n",
      "Train loss 0.6791 Train acc 47.0999 Val loss 0.6836 Val acc 45.283\n",
      "Train loss 0.6658 Train acc 58.84 Val loss 0.6694 Val acc 60.3774\n",
      "Train loss 0.6391 Train acc 68.0643 Val loss 0.6569 Val acc 62.2642\n",
      "Train loss 0.5962 Train acc 72.5367 Val loss 0.6291 Val acc 62.8931\n",
      "Train loss 0.5537 Train acc 74.0042 Val loss 0.6077 Val acc 66.6667\n",
      "Train loss 0.5014 Train acc 77.2886 Val loss 0.6091 Val acc 70.4403\n",
      "Train loss 0.4901 Train acc 79.2453 Val loss 0.6066 Val acc 69.8113\n",
      "Train loss 0.4641 Train acc 80.9224 Val loss 0.572 Val acc 73.5849\n",
      "Train loss 0.4381 Train acc 81.5514 Val loss 0.6166 Val acc 69.8113\n",
      "Train loss 0.422 Train acc 82.8092 Val loss 0.616 Val acc 71.0692\n",
      "Train loss 0.3958 Train acc 84.8358 Val loss 0.5881 Val acc 74.2138\n"
     ]
    }
   ],
   "source": [
    "best_val  = {\n",
    "    'epoch': 0,\n",
    "    'model_state_dict': 0,\n",
    "    'optimizer_state_dict': 0,\n",
    "    'loss': 0,\n",
    "    'acc':0\n",
    "}\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.0001)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "for epoch_index in range(200):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    # Iterate over training dataset\n",
    "    # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "    \n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size=len(dataset), device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # the training routine is these 5 steps:\n",
    "\n",
    "        # --------------------------------------\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # step 2. compute the output\n",
    "        y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # -----------------------------------------\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    # Iterate over val dataset\n",
    "    # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, batch_size=len(dataset), device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    \n",
    "    classifier.eval()\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # compute the output\n",
    "        y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "    \n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    if epoch_index%10 == 0:\n",
    "        print('Train loss', round(train_state['train_loss'][-1], 4), 'Train acc', round(train_state['train_acc'][-1], 4), 'Val loss', round(train_state['val_loss'][-1], 4), 'Val acc', round(train_state['val_acc'][-1], 4))\n",
    "    if(best_val['acc']<train_state['val_acc'][-1]):\n",
    "        best_val  = {\n",
    "            'epoch': epoch_index,\n",
    "            'model_state_dict': classifier.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss,\n",
    "            'acc': running_acc,\n",
    "        }\n",
    "    train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "torch.save(best_val, \"Glove.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "622b541e-e0d1-47e1-9cb0-e9e5da6368ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "checkpoint = torch.load(\"Glove.h5\")\n",
    "classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "classifier.eval()\n",
    "\n",
    "# classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('val')\n",
    "\n",
    "batch_generator = generate_batches(dataset, batch_size=len(dataset), device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "\n",
    "classifier.eval()\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e0d39e2-06a2-46f2-b34e-8eeb8cce1437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6257244944572449;\n",
      "Test Accuracy: 71.69811320754717\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81dd94f-25a3-41ee-b358-0c18ad12bda8",
   "metadata": {},
   "source": [
    "# Multi-class Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c8441f32-e7f0-4a99-bf70-eccc3d6995f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    injury_bodyparts_file=\"./data/injurybodyparts.csv\",\n",
    "    vectorizer_file=\"MCWBvectorizer.json\",\n",
    "    glove_filepath = \"./Glove/glove.6B.200d.txt\",\n",
    "    model_state_file = 'RNN.h5',\n",
    "    # Training hyperparameter\n",
    "    val_proportion=0.1,\n",
    "    test_proportion=0.2,\n",
    "    learning_rate = 0.001,\n",
    "    seed=666,\n",
    "    dropout_p=0.1,\n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    use_glove=True,\n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031d7ad-7f46-4265-864f-a71647cb395b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214e8da-368a-4608-8a3f-c02d43a3dfe7",
   "metadata": {},
   "source": [
    "After few Experements we found out padding at the font is much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "bd8dcb41-5e0c-4eee-98aa-850803188f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    \n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "class IBPVectorizer(object):\n",
    "    def __init__(self, NARRATIVE_vocab, target_vocab):\n",
    "        self.NARRATIVE_vocab = NARRATIVE_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def vectorize(self, NARRATIVE, vector_length=-1):\n",
    "        indices = [self.NARRATIVE_vocab.begin_seq_index]\n",
    "        indices.extend(self.NARRATIVE_vocab.lookup_token(token) for token in NARRATIVE.split(\" \"))\n",
    "        indices.append(self.NARRATIVE_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        \n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[vector_length - len(indices):] = indices\n",
    "        out_vector[:vector_length - len(indices)] = self.NARRATIVE_vocab.mask_index\n",
    "        return out_vector, vector_length\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cutoff=5):\n",
    "        target_vocab = Vocabulary()        \n",
    "        for target in sorted(set(df.target)):\n",
    "            target_vocab.add_token(target)\n",
    "        \n",
    "        word_counts = Counter()\n",
    "        for NARRATIVE in df.NARRATIVE:\n",
    "            for token in NARRATIVE.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "        \n",
    "        NARRATIVE_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                NARRATIVE_vocab.add_token(word)\n",
    "        \n",
    "        return cls(NARRATIVE_vocab, target_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_vocab = SequenceVocabulary.from_serializable(contents['NARRATIVE_vocab'])\n",
    "        category_vocab = Vocabulary.from_serializable(contents['target_vocab'])\n",
    "        return cls(NARRATIVE_vocab=NARRATIVE_vocab, target_vocab=target_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'NARRATIVE_vocab': self.NARRATIVE_vocab.to_serializable(), 'target_vocab': self.target_vocab.to_serializable()}\n",
    "\n",
    "class IBPDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, df.NARRATIVE)) + 2\n",
    "        \n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size), 'val': (self.val_df, self.validation_size), 'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        class_counts = df.target.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.target_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, news_csv):\n",
    "        df = pd.read_csv(news_csv)\n",
    "        train_df = df[df.split=='train']\n",
    "        return cls(df, IBPVectorizer.from_dataframe(train_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n",
    "        df = pd.read_csv(news_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(news_csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        NARRATIVE_vector, vector_length = self._vectorizer.vectorize(row.NARRATIVE, self._max_seq_length)\n",
    "        target_index = self._vectorizer.target_vocab.lookup_token(row.target) #-1\n",
    "        return {'x_data': NARRATIVE_vector,'y_target': target_index, 'x_length': vector_length}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9efde8c-b049-4b42-b098-bde49c38b7d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classfiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "75e20440-c80b-436f-813e-0800769fb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_gather(y_out, x_lengths):\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, column_index])\n",
    "\n",
    "    return torch.stack(out)\n",
    "\n",
    "class IBPClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_classes, rnn_hidden_size, hidden_dim, RNN=True, pretrained_embeddings=None, padding_idx=0, batch_first=True):\n",
    "        super(IBPClassifier, self).__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size, num_embeddings=num_embeddings, padding_idx=padding_idx)        \n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size, num_embeddings=num_embeddings, padding_idx=padding_idx,  _weight=pretrained_embeddings)\n",
    "\n",
    "        if RNN:\n",
    "            self.rnn = VanillaRNN(input_size=embedding_size, hidden_size=rnn_hidden_size, batch_first=batch_first)\n",
    "        else:\n",
    "            self.rnn = LstmRNN(input_size=embedding_size, hidden_size=rnn_hidden_size, batch_first=batch_first)\n",
    "#             self.rnn = nn.LSTM(embedding_size, rnn_hidden_size, batch_first=batch_first) \n",
    "        \n",
    "        self.RNN = RNN\n",
    "        self.num_classes = num_classes\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        #MLP         \n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, hidden_dim[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], num_classes)\n",
    "\n",
    "    def forward(self, x, x_lengths=None, apply_softmax=False):\n",
    "        # embed and permute so features are channels\n",
    "        out = self.emb(x)\n",
    "        out = self.rnn(out)\n",
    "\n",
    "        \n",
    "        if self.RNN:\n",
    "            if x_lengths is not None:\n",
    "                out = column_gather(out, x_lengths)\n",
    "            else:\n",
    "                out = out[:, -1, :]\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "\n",
    "        if apply_softmax:\n",
    "            out = torch.nn.functional.softmax(out, dim=1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e2815-3fd9-43f0-a046-63102382f501",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889ec42-fd31-4359-98ba-5b934a467292",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "140924c0-dac2-4d38-a525-fcdf29439348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.batch_first = batch_first\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def _initial_hidden(self, batch_size):\n",
    "        return torch.zeros((batch_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, x_in, initial_hidden=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "    \n",
    "        hiddens = []\n",
    "\n",
    "        if initial_hidden is None:\n",
    "            initial_hidden = self._initial_hidden(batch_size)\n",
    "            initial_hidden = initial_hidden.to(x_in.device)\n",
    "\n",
    "        hidden_t = initial_hidden\n",
    "                    \n",
    "        for t in range(seq_size):\n",
    "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
    "            hiddens.append(hidden_t)\n",
    "            \n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f405ca4-c202-46d3-9de8-8c84a394c3ea",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f4a5b0aa-157f-42e5-b34e-020a1fc493b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = IBPDataset.load_dataset_and_load_vectorizer(args.injury_bodyparts_file, args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = IBPDataset.load_dataset_and_make_vectorizer(args.injury_bodyparts_file)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.NARRATIVE_vocab._token_to_idx.keys()\n",
    "    embeddings, embedding_size = make_embedding_matrix(glove_filepath=args.glove_filepath, words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "78193074-e47c-4b42-aa97-71821306f942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d019b4a5-8a68-4525-804b-f830fbf9d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = IBPClassifier(\n",
    "    embedding_size=embedding_size, \n",
    "    num_embeddings=len(vectorizer.NARRATIVE_vocab),\n",
    "    num_classes=len(vectorizer.target_vocab),\n",
    "    rnn_hidden_size=512,\n",
    "    hidden_dim = [128, 64],\n",
    "    padding_idx=vectorizer.NARRATIVE_vocab.mask_index,\n",
    "    pretrained_embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a7c9eed5-1367-458c-a7dd-97767010180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.6119 Train acc 6.4925 Val loss 1.6131 Val acc 6.383\n",
      "Train loss 1.5998 Train acc 16.3895 Val loss 1.6041 Val acc 14.8936\n",
      "Train loss 1.5783 Train acc 30.2454 Val loss 1.5865 Val acc 24.1135\n",
      "Train loss 1.5176 Train acc 40.5384 Val loss 1.5151 Val acc 38.2979\n",
      "Train loss 1.4725 Train acc 41.9636 Val loss 1.4954 Val acc 39.7163\n",
      "Train loss 1.4131 Train acc 48.9311 Val loss 1.434 Val acc 45.3901\n",
      "Train loss 1.3488 Train acc 41.4885 Val loss 1.2987 Val acc 54.6099\n",
      "Train loss 1.2416 Train acc 53.6817 Val loss 1.2454 Val acc 48.227\n",
      "Train loss 1.1366 Train acc 54.9485 Val loss 1.1298 Val acc 56.7376\n",
      "Train loss 1.0602 Train acc 59.4616 Val loss 1.0812 Val acc 56.0284\n",
      "Train loss 0.9745 Train acc 59.7783 Val loss 1.0191 Val acc 60.2837\n",
      "Train loss 0.8832 Train acc 65.3207 Val loss 0.9567 Val acc 65.2482\n",
      "Train loss 0.8488 Train acc 68.4877 Val loss 0.9567 Val acc 63.1206\n",
      "Train loss 0.793 Train acc 67.5376 Val loss 0.9572 Val acc 59.5745\n",
      "Train loss 0.7472 Train acc 71.2589 Val loss 0.9934 Val acc 62.4113\n",
      "Train loss 0.6854 Train acc 70.1504 Val loss 0.9595 Val acc 65.9574\n",
      "Train loss 0.5952 Train acc 77.0388 Val loss 0.9086 Val acc 66.6667\n",
      "Train loss 0.5446 Train acc 79.5724 Val loss 0.9432 Val acc 67.3759\n",
      "Train loss 0.488 Train acc 80.2059 Val loss 0.9438 Val acc 70.922\n",
      "Train loss 0.4343 Train acc 83.5313 Val loss 0.9459 Val acc 72.3404\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.0001)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "best_val  = {\n",
    "    'epoch': 0,\n",
    "    'model_state_dict': 0,\n",
    "    'optimizer_state_dict': 0,\n",
    "    'loss': 0,\n",
    "    'acc':0\n",
    "}\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(200):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=len(dataset), device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------    \n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x=batch_dict['x_data'], x_lengths=batch_dict['x_length'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    \n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\\\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, batch_size=len(dataset), device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = classifier(x=batch_dict['x_data'], x_lengths=batch_dict['x_length'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        if(best_val['acc']<train_state['val_acc'][-1]):\n",
    "            best_val  = {\n",
    "                'epoch': epoch_index,\n",
    "                'model_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': running_loss,\n",
    "                'acc': running_acc,\n",
    "            }\n",
    "\n",
    "        if epoch_index%10 == 0:\n",
    "            print('Train loss', round(train_state['train_loss'][-1], 4), 'Train acc', round(train_state['train_acc'][-1], 4), 'Val loss', round(train_state['val_loss'][-1], 4), 'Val acc', round(train_state['val_acc'][-1], 4))\n",
    "            \n",
    "        train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n",
    "torch.save(best_val, \"RNNGlove.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3c0c0499-2a0e-4e18-bd5f-add62f16d813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.0190712213516235;\n",
      "Test Accuracy: 74.46808510638297\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"RNNGlove.h5\")\n",
    "classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "classifier.eval()\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'], x_lengths=batch_dict['x_length'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n",
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4cd8b5-cc1b-416f-98e9-c68de0e83fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc10fb20-e4e6-4822-9814-7fcd765fb9f6",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7c885b83-b760-4dd1-8bb4-40575d90f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=True):\n",
    "        super(LstmRNN, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=batch_first) \n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.batch_first:\n",
    "            batch_first_output, (self.hidden, self.cell) = self.lstm(x)\n",
    "        else:\n",
    "            batch_first_output, (self.hidden, self.cell) = self.lstm(x)\n",
    "        return batch_first_output[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9315988c-fadd-4c47-a4f9-f7e737cac91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = IBPClassifier(\n",
    "    embedding_size=embedding_size, \n",
    "    num_embeddings=len(vectorizer.NARRATIVE_vocab),\n",
    "    num_classes=len(vectorizer.target_vocab),\n",
    "    rnn_hidden_size=512,\n",
    "    hidden_dim = [256, 64],\n",
    "    RNN=False,\n",
    "    padding_idx=vectorizer.NARRATIVE_vocab.mask_index,\n",
    "    pretrained_embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0b19d42a-aa4c-4287-9682-1dc895f1e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.6112 Train acc 21.7736 Val loss 1.6105 Val acc 21.9858\n",
      "Train loss 1.6062 Train acc 21.7736 Val loss 1.606 Val acc 21.9858\n",
      "Train loss 1.5979 Train acc 32.2249 Val loss 1.5974 Val acc 33.3333\n",
      "Train loss 1.5762 Train acc 37.3713 Val loss 1.5742 Val acc 34.7518\n",
      "Train loss 1.5025 Train acc 34.2835 Val loss 1.4583 Val acc 35.461\n",
      "Train loss 1.4033 Train acc 40.9343 Val loss 1.3786 Val acc 48.227\n",
      "Train loss 1.306 Train acc 46.0808 Val loss 1.2405 Val acc 56.0284\n",
      "Train loss 1.1694 Train acc 58.8282 Val loss 1.183 Val acc 56.7376\n",
      "Train loss 1.0378 Train acc 65.7165 Val loss 1.1411 Val acc 62.4113\n",
      "Train loss 0.9124 Train acc 66.0333 Val loss 1.0685 Val acc 64.539\n",
      "Train loss 0.7956 Train acc 71.6548 Val loss 1.0085 Val acc 67.3759\n",
      "Train loss 0.7246 Train acc 74.901 Val loss 0.9703 Val acc 67.3759\n",
      "Train loss 0.6182 Train acc 77.1971 Val loss 0.9625 Val acc 70.2128\n",
      "Train loss 0.5634 Train acc 77.5139 Val loss 0.9201 Val acc 74.4681\n",
      "Train loss 0.48 Train acc 82.977 Val loss 0.9213 Val acc 69.5035\n",
      "Train loss 0.4263 Train acc 85.194 Val loss 0.9453 Val acc 70.922\n",
      "Train loss 0.3697 Train acc 86.4608 Val loss 1.0041 Val acc 72.3404\n",
      "Train loss 0.3142 Train acc 88.5194 Val loss 1.0079 Val acc 73.0496\n",
      "Train loss 0.2875 Train acc 90.4196 Val loss 0.9858 Val acc 71.6312\n",
      "Train loss 0.4278 Train acc 84.2439 Val loss 1.1007 Val acc 71.6312\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.0001)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "best_val  = {\n",
    "    'epoch': 0,\n",
    "    'model_state_dict': 0,\n",
    "    'optimizer_state_dict': 0,\n",
    "    'loss': 0,\n",
    "    'acc':0\n",
    "}\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(200):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=len(dataset), device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------    \n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x=batch_dict['x_data'], x_lengths=batch_dict['x_length'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    \n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\\\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, batch_size=len(dataset), device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = classifier(x=batch_dict['x_data'], x_lengths=batch_dict['x_length'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        if(best_val['acc']<train_state['val_acc'][-1]):\n",
    "            best_val  = {\n",
    "                'epoch': epoch_index,\n",
    "                'model_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': running_loss,\n",
    "                'acc': running_acc,\n",
    "            }\n",
    "\n",
    "        if epoch_index%10 == 0:\n",
    "            print('Train loss', round(train_state['train_loss'][-1], 4), 'Train acc', round(train_state['train_acc'][-1], 4), 'Val loss', round(train_state['val_loss'][-1], 4), 'Val acc', round(train_state['val_acc'][-1], 4))\n",
    "            \n",
    "        train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n",
    "torch.save(best_val, \"LSTMGlove.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "94c88e90-5bce-4300-addb-769c22c1cf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.0109914541244507;\n",
      "Test Accuracy: 74.46808510638297\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"LSTMGlove.h5\")\n",
    "classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "classifier.eval()\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'], x_lengths=batch_dict['x_length'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n",
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b93c56-8335-45c4-baad-7a15167fe595",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b34cae-0b1d-47df-b869-a8cc425a826b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5897a0-e467-41f9-ac90-32b633cdd6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4ebee4a-b42f-45f8-8597-4564290a0255",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9105157-14d8-42f7-8722-612cab5abbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f6d0eb7-94e0-4c23-9560-a2b71feff7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df['NARRATIVE'].str.len() < 30)\n",
    "df.loc[mask]['NARRATIVE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749c56c-8001-4982-8171-06f731be0fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
