{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee82352e-5553-48e5-a623-611146796ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing as mp\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from argparse import Namespace\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c6041-e4e1-4567-ae7a-20fb9dac9b5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3746352-af85-4ef2-81c9-2e696aecf858",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not nltk.find('corpora/wordnet'):\n",
    "     nltk.download('wordnet')\n",
    "porter_stemmer  = PorterStemmer()\n",
    "lemmatizer      = WordNetLemmatizer()\n",
    "regex_tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "spell  = SpellChecker()\n",
    "one_hot_vectorizer = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a43282ee-9d1a-443d-a4cc-c69b4a7e3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    degree_injury_file=\"./data/degreeinjury.csv\",\n",
    "    injury_bodyparts_file=\"./data/injurybodyparts.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.h5\",\n",
    "    save_dir=\"model_storage\\document_classification\",\n",
    "    # Model hyper parameters\n",
    "    use_glove=False,\n",
    "    embedding_size=100, \n",
    "    # Training hyper parameter\n",
    "    window_size=5,\n",
    "    val_proportion=0.1,\n",
    "    test_proportion=0.2,\n",
    "    learning_rate = 0.001,\n",
    "    seed=666,\n",
    "    dropout_p=0.1, \n",
    "    batch_size=256, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd08af5-d4b0-416b-a9c8-8e7306df0125",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "373dae4e-bd9c-4c88-bded-888e4eb1a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "def load_glove_from_file(glove_filepath):\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, encoding=\"utf8\") as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings\n",
    "\n",
    "def clean_text(text):\n",
    "    txt = ''\n",
    "    txt = txt.strip()\n",
    "    txt = re.sub(r\"([.,!?])\", r\" \\1 \", txt)\n",
    "    for word in tokenize(text):\n",
    "        # The EE is short hand on employee\n",
    "        if word=='ee' or word == 'EE':\n",
    "            txt += ' employee '\n",
    "        elif word in string.punctuation:\n",
    "            txt = txt + ' ' +word\n",
    "        elif len(word)>2:\n",
    "            word = spell.correction(word)\n",
    "            txt = txt + ' ' + word.lower().strip()\n",
    "    txt = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", txt)\n",
    "    return txt.strip()\n",
    "\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(words, df=False):\n",
    "    stemmed_words=[porter_stemmer.stem(word) for word in words]\n",
    "    if df:\n",
    "        return pd.DataFrame({'original': words,'stemmed': stemmed_words})\n",
    "    return stemmed_words\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize(words, df=False):\n",
    "    lemmatized_words=[]\n",
    "    tagged_sent = nltk.pos_tag(words)\n",
    "    \n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or nltk.corpus.wordnet.NOUN\n",
    "        if wordnet_pos is None:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(tag[0]))\n",
    "        else:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(tag[0], pos=wordnet_pos))\n",
    "    \n",
    "    if df:\n",
    "        return pd.DataFrame({'original': words,'lemmatized': lemmatized_words})\n",
    "    return lemmatized_words\n",
    "\n",
    "def SK_TFIDF_stopwords(corpus, vectorizer):\n",
    "    vectorizer.fit(corpus)\n",
    "    X = vectorizer.transform(corpus)\n",
    "    return vectorizer.stop_words_, X\n",
    "\n",
    "def get_words(texts):\n",
    "    words = []\n",
    "    for text in texts:\n",
    "        words+=regex_tokenizer.tokenize(text)\n",
    "    return np.asarray(words).reshape(-1, 1)\n",
    "\n",
    "def OHE_2D(sent, sparse=False):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse=sparse)\n",
    "    words=[]\n",
    "    for w in sent.values:\n",
    "        words += w\n",
    "    X = enc.fit(np.array(words).reshape(-1,1))\n",
    "    \n",
    "    # Round up to without unit digit\n",
    "    ohe = sent.apply(lambda x: enc.transform(np.array(x).reshape(-1,1)))\n",
    "    maxwords = max([len(x) for x in ohe])\n",
    "    maxwords = math.ceil(maxwords/10)*10\n",
    "    dim = len(ohe[0][0])\n",
    "    print('Vector length:          ', dim, '\\nMaximum number of words:', maxwords)\n",
    "\n",
    "    return [np.concatenate((x, np.zeros((maxwords - len(x), dim))), axis=0) for x in ohe]\n",
    "\n",
    "def OHE_1D(sent):\n",
    "    corpus = [' '.join(x) for x in sent.values]\n",
    "    print(corpus[0])\n",
    "    ohe = CountVectorizer(binary=True)\n",
    "    return pd.DataFrame(ohe.fit_transform(corpus).todense(), columns=ohe.get_feature_names())\n",
    "\n",
    "def spell_check(words):\n",
    "    return [spell.correction(word) for word in words]\n",
    "\n",
    "def remove_stopwords(words, stopwords):\n",
    "    return [w for w in words if w not in stopwords]\n",
    "\n",
    "def split_save(data, path, test=0.2, valid= 0.1):\n",
    "    train, test  = train_test_split(data,  test_size=test, random_state=666, shuffle=True, stratify=data['target'])\n",
    "    train, valid = train_test_split(train, test_size=valid, random_state=666, shuffle=True, stratify=train['target'])\n",
    "    train['split'] = 'train'\n",
    "    test['split']  = 'test'\n",
    "    valid['split'] = 'val'\n",
    "    pd.concat([train, valid, test]).to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f187493-c195-4b74-a791-0e9ba3907a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Narrative is computionally expensive run if only necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4dde7-dae6-4523-b000-93ba6186655d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Clean Narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c7b9e2af-9940-49c7-92c3-515d1cb781cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEGREE_INJURY</th>\n",
       "      <th>DEGREE_INJURY_CD</th>\n",
       "      <th>INJ_BODY_PART_CD</th>\n",
       "      <th>INJ_BODY_PART</th>\n",
       "      <th>NARRATIVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DAYS RESTRICTED ACTIVITY ONLY</td>\n",
       "      <td>5</td>\n",
       "      <td>700</td>\n",
       "      <td>MULTIPLE PARTS (MORE THAN ONE MAJOR)</td>\n",
       "      <td>Employee was cleaning up at the Primary Crushe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   DEGREE_INJURY DEGREE_INJURY_CD INJ_BODY_PART_CD  \\\n",
       "0  DAYS RESTRICTED ACTIVITY ONLY                5              700   \n",
       "\n",
       "                          INJ_BODY_PART  \\\n",
       "0  MULTIPLE PARTS (MORE THAN ONE MAJOR)   \n",
       "\n",
       "                                           NARRATIVE  \n",
       "0  Employee was cleaning up at the Primary Crushe...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('us_data_2000.csv')\n",
    "df = df[['DEGREE_INJURY', 'DEGREE_INJURY_CD', 'INJ_BODY_PART_CD', 'INJ_BODY_PART', 'NARRATIVE']]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa999f8c-7ac3-4509-9ba1-e48b37b2f51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEGREE_INJURY       False\n",
       "DEGREE_INJURY_CD    False\n",
       "INJ_BODY_PART_CD    False\n",
       "INJ_BODY_PART       False\n",
       "NARRATIVE           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a409ba55-0332-4b0c-b284-f53034bad9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NARRATIVE NaN count:    0\n",
      "NARRATIVE len>10 count: 1\n",
      "\n",
      "DEGREE_INJURY_CD \"?\" count:  11\n",
      "\n",
      "INJ_BODY_PART_CD \"?\" count:  242\n"
     ]
    }
   ],
   "source": [
    "print('NARRATIVE NaN count:   ', df[df['NARRATIVE'].isna()].shape[0])\n",
    "print('NARRATIVE len>10 count:', df[df['NARRATIVE'].str.len() < 10].shape[0])\n",
    "print('\\nDEGREE_INJURY_CD \"?\" count: ', df[df['DEGREE_INJURY_CD'] == '?'].shape[0])\n",
    "print('\\nINJ_BODY_PART_CD \"?\" count: ', df[df['INJ_BODY_PART_CD'] == '?'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344ae3a-1e8c-4ac7-a56a-5f327fb664f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove Narrative are too short and NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1531e8db-f9c6-434c-afb2-754c279b0b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin data size:       2000\n",
      "Clean data size: 1999\n"
     ]
    }
   ],
   "source": [
    "print(\"Origin data size:      \", df.shape[0])\n",
    "df = df[df['NARRATIVE'].notna()]\n",
    "df = df[df['NARRATIVE'].str.len() > 10]\n",
    "print(\"Clean data size:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc34fe-6333-4ef7-b23d-56376feafb19",
   "metadata": {
    "tags": []
   },
   "source": [
    "### To Lower case and correct words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "72accc8a-fe28-4502-bcd1-ea266391ad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'employee was cleaning the primary crusher with the dingo skid steer . the employee slipped and fell while operating the skid steer and the machine pinned him against the cement retaining wall .'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    df['NARRATIVE'] = pool.map(clean_text, df['NARRATIVE'])\n",
    "df['NARRATIVE'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee25e3-3289-45af-8925-fc9fb978cc4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset for task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "79ee30f7-43ad-4c41-b34e-9bfefdcdf054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEGREE_INJURY_CD</th>\n",
       "      <th>DEGREE_INJURY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ACCIDENT ONLY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FATALITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PERM TOT OR PERM PRTL DISABLTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DAYS AWAY FROM WORK ONLY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>DYS AWY FRM WRK &amp; RESTRCTD ACT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>DAYS RESTRICTED ACTIVITY ONLY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>NO DYS AWY FRM WRK,NO RSTR ACT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>OCCUPATNAL ILLNESS NOT DEG 1-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>INJURIES DUE TO NATURAL CAUSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>INJURIES INVOLVNG NONEMPLOYEES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>ALL OTHER CASES (INCL 1ST AID)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DEGREE_INJURY_CD                   DEGREE_INJURY\n",
       "0                  0                   ACCIDENT ONLY\n",
       "1                  1                        FATALITY\n",
       "2                  2  PERM TOT OR PERM PRTL DISABLTY\n",
       "3                  3        DAYS AWAY FROM WORK ONLY\n",
       "4                  4  DYS AWY FRM WRK & RESTRCTD ACT\n",
       "5                  5   DAYS RESTRICTED ACTIVITY ONLY\n",
       "6                  6  NO DYS AWY FRM WRK,NO RSTR ACT\n",
       "7                  7  OCCUPATNAL ILLNESS NOT DEG 1-6\n",
       "8                  8  INJURIES DUE TO NATURAL CAUSES\n",
       "9                  9  INJURIES INVOLVNG NONEMPLOYEES\n",
       "10                10  ALL OTHER CASES (INCL 1ST AID)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_degree=df[['NARRATIVE', 'DEGREE_INJURY_CD', 'DEGREE_INJURY']].copy()\n",
    "df_degree = df_degree[df_degree['DEGREE_INJURY_CD'] != '?']\n",
    "df_degree['DEGREE_INJURY_CD'] = pd.to_numeric(df_degree['DEGREE_INJURY_CD'])\n",
    "\n",
    "def get_degree_injury(dataframe):\n",
    "    CD = []\n",
    "    INJURY = []\n",
    "    for i in range(0, len(dataframe.DEGREE_INJURY_CD.unique())):\n",
    "        CD.append(i)\n",
    "        INJURY.append(dataframe[dataframe['DEGREE_INJURY_CD'] == i]['DEGREE_INJURY'].values[0])\n",
    "    return pd.DataFrame({'DEGREE_INJURY_CD': CD,'DEGREE_INJURY': INJURY})\n",
    "\n",
    "get_degree_injury(df_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "19dd8f6c-422e-4b74-91a4-ccb53a12e201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAYS AWAY FROM WORK ONLY          29.879276\n",
       "NO DYS AWY FRM WRK,NO RSTR ACT    27.766600\n",
       "DAYS RESTRICTED ACTIVITY ONLY     18.058350\n",
       "ACCIDENT ONLY                     11.016097\n",
       "DYS AWY FRM WRK & RESTRCTD ACT     7.293763\n",
       "OCCUPATNAL ILLNESS NOT DEG 1-6     2.867203\n",
       "ALL OTHER CASES (INCL 1ST AID)     1.006036\n",
       "PERM TOT OR PERM PRTL DISABLTY     0.905433\n",
       "FATALITY                           0.553320\n",
       "INJURIES DUE TO NATURAL CAUSES     0.503018\n",
       "INJURIES INVOLVNG NONEMPLOYEES     0.150905\n",
       "Name: DEGREE_INJURY, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*df_degree['DEGREE_INJURY'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9053eed8-431c-44e9-aa96-69dd42f575fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = [\n",
    "    'DAYS AWAY FROM WORK ONLY',\n",
    "    'DAYS RESTRICTED ACTIVITY ONLY',\n",
    "    'DYS AWY FRM WRK & RESTRCTD ACT'\n",
    "]\n",
    "df_degree['target'] = df_degree['DEGREE_INJURY'].isin(group1)\n",
    "df_degree[\"target\"] = df_degree[\"target\"].astype(str)\n",
    "del df_degree['DEGREE_INJURY']\n",
    "del df_degree['DEGREE_INJURY_CD']\n",
    "split_save(df_degree, path=args.degree_injury_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c74cf-db27-4aee-8057-8171206b8e5f",
   "metadata": {},
   "source": [
    "### Dataset for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9296b6ce-4dbd-42ca-8eb9-0bcc92d1b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD     = ['SKULL', 'EAR(S) INTERNAL & HEARING', 'EAR(S) EXTERNAL', 'EAR(S) INTERNAL & EXTERNAL' ,'EYE(S) OPTIC NERVE/VISON', 'NOSE/NASAL PASSAGES/SINUS/SMELL', 'EAR(S) INTERNAL & HEARING', 'BRAIN', 'FACE,NEC', 'FACE, MULTIPLE PARTS', 'HEAD, MULTIPLE PARTS', 'HEAD,NEC', 'MULTIPLE PARTS', 'MOUTH/LIP/TEETH/TONGUE/THROAT/TASTE', 'JAW INCLUDE CHIN', 'SCALP']\n",
    "LEG      = ['LEG, MULTIPLE PARTS', 'LOWER LEG/TIBIA/FIBULA', 'FOOT(NOT ANKLE/TOE)/TARSUS/METATARSUS', 'LEG, NEC', 'KNEE/PATELLA', 'TOE(S)/PHALANGES', 'ANKLE', 'THIGH/FEMUR', 'LOWER EXTREMITIES,NEC', 'LOWER EXTREMITIES, MULTIPLE PARTS']\n",
    "ARM      = ['ARM, MULTIPLE PARTS', 'ARM,NEC', 'HAND (NOT WRIST OR FINGERS)', 'WRIST', 'FINGER(S)/THUMB', 'UPPER ARM/HUMERUS', 'SHOULDERS (COLLARBONE/CLAVICLE/SCAPULA)', 'ELBOW', 'FOREARM/ULNAR/RADIUS', 'UPPER EXTREMITIES, MULTIPLE', 'UPPER EXTREMITIES, NEC']\n",
    "BODY     = ['CHEST (RIBS/BREAST BONE/CHEST ORGNS)', 'BODY SYSTEMS', 'ABDOMEN/INTERNAL ORGANS', 'BODY PARTS, NEC', 'TRUNK, MULTIPLE PARTS', 'TRUNK,NEC', 'HIPS (PELVIS/ORGANS/KIDNEYS/BUTTOCKS)']\n",
    "BACK     = ['BACK (MUSCLES/SPINE/S-CORD/TAILBONE)']\n",
    "MULTIPLE = ['MULTIPLE PARTS (MORE THAN ONE MAJOR)']\n",
    "\n",
    "df_parts=df[['NARRATIVE', 'INJ_BODY_PART_CD', 'INJ_BODY_PART']].copy()\n",
    "df_parts = df_parts[df_parts['INJ_BODY_PART_CD'] != '?']\n",
    "del df_parts['INJ_BODY_PART_CD']\n",
    "\n",
    "df_parts['INJ_BODY_PART'][df_parts['INJ_BODY_PART'].isin(HEAD)]     = 'Head'\n",
    "df_parts['INJ_BODY_PART'][df_parts['INJ_BODY_PART'].isin(LEG)]      = 'Leg'\n",
    "df_parts['INJ_BODY_PART'][df_parts['INJ_BODY_PART'].isin(ARM)]      = 'Arm'\n",
    "df_parts['INJ_BODY_PART'][df_parts['INJ_BODY_PART'].isin(BODY)]     = 'Body'\n",
    "df_parts['INJ_BODY_PART'][df_parts['INJ_BODY_PART'].isin(BACK)]     = 'Back'\n",
    "df_parts['INJ_BODY_PART'][df_parts['INJ_BODY_PART'].isin(MULTIPLE)] = 'Multiple Parts'\n",
    "df_parts['target'] = df_parts['INJ_BODY_PART']\n",
    "del df_parts['INJ_BODY_PART']\n",
    "split_save(df_degree, path=args.injury_bodyparts_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb361bb-cfd7-46eb-8812-265b347bed1b",
   "metadata": {},
   "source": [
    "### Dataset for task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4ca632e-961a-474c-909d-2cc63bcd7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e81ab-b43f-4682-9ad6-a2e612bb7877",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Prepreocessing and Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e6ae0e-c8ae-475f-bbd0-8f2ff1253ce9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test lem and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "32b354e0-d4d8-4957-ac9a-f84d078a8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem      = stem(nltk.tokenize.word_tokenize((df['NARRATIVE'].values[0])), df=True)\n",
    "df_lemmatize = lemmatize(nltk.tokenize.word_tokenize((df['NARRATIVE'].values[0])), df=True)\n",
    "df_stem['lemmatized'] = df_lemmatize['lemmatized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ae202934-1156-47ff-b8c6-0f6e04a3a9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>employee</td>\n",
       "      <td>employe</td>\n",
       "      <td>employee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>wa</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>primary</td>\n",
       "      <td>primari</td>\n",
       "      <td>primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>employee</td>\n",
       "      <td>employe</td>\n",
       "      <td>employee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>operating</td>\n",
       "      <td>oper</td>\n",
       "      <td>operate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>machine</td>\n",
       "      <td>machin</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     original  stemmed lemmatized\n",
       "0    employee  employe   employee\n",
       "1         was       wa         be\n",
       "4     primary  primari    primary\n",
       "13   employee  employe   employee\n",
       "18  operating     oper    operate\n",
       "24    machine   machin    machine"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stem[df_stem['stemmed'] != df_stem['lemmatized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a263af8-78cf-464c-a1d8-7da557d0f946",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b7c82ddc-184e-4ae9-bbf7-d0656d73ce14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 19\n",
      "Number of terms    : 3105\n",
      "Ascending\n",
      "               term      rank\n",
      "1893  perpendicular  0.144312\n",
      "198            babi  0.150171\n",
      "36       acupunctur  0.150171\n",
      "3006             wd  0.151109\n",
      "2909    unknowingli  0.151109\n",
      "2401        silicon  0.158104\n",
      "2813            tow  0.158681\n",
      "1860       particip  0.159118\n",
      "904         exercis  0.159118\n",
      "1302            ind  0.163037\n",
      "2422            sky  0.167821\n",
      "1038          forth  0.167821\n",
      "825          easier  0.167821\n",
      "2847         tricep  0.168713\n",
      "2809           toss  0.168713\n",
      "\n",
      "Descending\n",
      "        term       rank\n",
      "938     fall  52.162024\n",
      "976   finger  43.838449\n",
      "633      cut  43.770485\n",
      "2232    rock  42.809985\n",
      "2240    roof  42.396292\n",
      "306     bolt  41.885435\n",
      "2568    step  37.096267\n",
      "1021    foot  35.736065\n",
      "1813     out  35.322635\n",
      "2607  strike  34.484985\n",
      "1845    pain  34.337641\n",
      "1914    piec  34.113770\n",
      "2854   truck  32.382425\n",
      "958     felt  31.701446\n",
      "2725    that  31.464384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cun/anaconda3/envs/CITS4012/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.13, min_df=1)\n",
    "_df = df[[\"NARRATIVE\"]].copy()\n",
    "_df['NARRATIVE'] = _df['NARRATIVE'].apply(lambda x: tokenize(x))\n",
    "_df['NARRATIVE'] = _df['NARRATIVE'].apply(lambda x: lemmatize(x))\n",
    "_df['NARRATIVE'] = _df['NARRATIVE'].apply(lambda x: stem(x))\n",
    "\n",
    "stopwrods, X = SK_TFIDF_stopwords(_df['NARRATIVE'].apply(lambda x: ' '.join(x)), vectorizer)\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# sum tfidf frequency of each term through documents\n",
    "sums = X.sum(axis=0)\n",
    "\n",
    "# connecting term to its sums frequency\n",
    "data = []\n",
    "for col, term in enumerate(terms):\n",
    "    data.append( (term, sums[0,col] ))\n",
    "print('Number of stopwords:', len(stopwrods))\n",
    "print('Number of terms    :', len(terms))\n",
    "\n",
    "ranking = pd.DataFrame(data, columns=['term','rank'])\n",
    "print('Ascending')\n",
    "print(ranking.sort_values('rank', ascending=True)[0:15])\n",
    "\n",
    "print('\\nDescending')\n",
    "print(ranking.sort_values('rank', ascending=False)[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8d03ca7d-7746-4856-8b84-5e4b851bbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in ['abb','abc','dia', 'yes','yee', 'that']:\n",
    "        stopwrods.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f21a3513-e07f-4ee0-a450-9db0e40ea989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'work', 'when', 'the', 'hi', 'that', 'and', 'right', 'have', 'hand', 'be', 'off', 'with', 'employe', 'caus', 'yes', 'abb', 'fell', 'slip', 'left', 'back', 'abc', 'while', 'from', 'yee', 'dia'}\n"
     ]
    }
   ],
   "source": [
    "print(stopwrods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051b166-3e6e-40fc-9c08-e32969f34d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f780bb66-4bb3-467e-931d-f5aeace0248a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Function repeative Prepreocessing and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1a8028de-ca99-45a2-b3d2-b2dada480db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_lem_sem(text):\n",
    "    words = tokenize(text)\n",
    "    words = lemmatize(words)\n",
    "    words = stem(words)\n",
    "    return words\n",
    "\n",
    "def removestop_join(words, stopwords):\n",
    "    words = remove_stopwords(words, stopwords)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def prepreocessing(data, stop, target=\"NARRATIVE\"):\n",
    "    _df=data.copy()\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        _df[target] = pool.map(tok_lem_sem, _df[target])\n",
    "    _df[target] = _df.apply(lambda row: removestop_join(row[target], stop), axis=1)\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5d510aee-36c4-4431-a494-55886e604e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepreocessing(pd.read_csv(args.degree_injury_file), stopwrods).to_csv(args.degree_injury_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "84e1d04a-04bd-46d5-b006-be354fafe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepreocessing(pd.read_csv(args.injury_bodyparts_file), stopwrods).to_csv(args.injury_bodyparts_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc13934-5ed9-49aa-ae8f-1784a91506a1",
   "metadata": {},
   "source": [
    "# Binary Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e37347-1b5d-4d94-a2d7-6555776adb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OHEVocabulary(object):\n",
    "    \"\"\"Class to process text and extract Vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                                for token, idx in self._token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = 1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx, 'add_unk': self._add_unk, 'unk_token': self._unk_token}\n",
    "    @classmethod\n",
    "    \n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "class OHESequenceVocabulary(OHEVocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(OHESequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(OHESequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "class OHEVectorizer(object):\n",
    "    def __init__(self, NARRATIVE_vocab, target_vocab):\n",
    "        self.NARRATIVE_vocab = NARRATIVE_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def vectorize(self, NARRATIVE):\n",
    "        one_hot = np.zeros(len(self.NARRATIVE_vocab), dtype=np.float32)\n",
    "        for token in NARRATIVE.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.NARRATIVE_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cutoff=25):\n",
    "        target_vocab = OHEVocabulary()        \n",
    "        for target in sorted(set(df.target)):\n",
    "            target_vocab.add_token(target)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for NARRATIVE in df.NARRATIVE:\n",
    "            for token in NARRATIVE.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "        \n",
    "        NARRATIVE_vocab = OHESequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                NARRATIVE_vocab.add_token(word)\n",
    "        \n",
    "        return cls(NARRATIVE_vocab, target_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        NARRATIVE_vocab =   OHESequenceVocabulary.from_serializable(contents['NARRATIVE_vocab'])\n",
    "        target_vocab =  OHEVocabulary.from_serializable(contents['target_vocab'])\n",
    "\n",
    "        return cls(NARRATIVE_vocab=NARRATIVE_vocab, target_vocab=target_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'NARRATIVE_vocab': self.NARRATIVE_vocab.to_serializable(),\n",
    "                'target_vocab': self.target_vocab.to_serializable()}\n",
    "\n",
    "class OHEDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, df.NARRATIVE)) + 2\n",
    "        \n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),'val': (self.val_df, self.validation_size),'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, csv):\n",
    "        df = pd.read_csv(csv)\n",
    "        return cls(df, OHEVectorizer.from_dataframe(df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n",
    "        df = pd.read_csv(news_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(news_csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        NARRATIVE_vector = self._vectorizer.vectorize(row.NARRATIVE)\n",
    "        target_index = self._vectorizer.target_vocab.lookup_token(row.target)-1\n",
    "        return {'x_data': NARRATIVE_vector,'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "class OHEClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim=1):\n",
    "        super(OHEClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dims[2], 1) \n",
    "    \n",
    "    def forward(self, x, apply_sigmoid=False):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        out = self.fc4(out)\n",
    "        \n",
    "        if apply_sigmoid:\n",
    "            out = torch.sigmoid(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d02dd841-eced-418f-9d49-20606a00fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = make_train_state(args)\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# dataset and vectorizer\n",
    "dataset = OHEDataset.load_dataset_and_make_vectorizer(args.degree_injury_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "# model\n",
    "classifier = OHEClassifier(input_dim=len(vectorizer.NARRATIVE_vocab), hidden_dims=[256, 256, 128])\n",
    "classifier = classifier.to(args.device)\n",
    "# loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.0001)\n",
    "train_state = make_train_state(args)\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6540ca4-c914-4033-8530-d2398db70d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.NARRATIVE_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "854b2a14-15c1-4ec0-aa21-b616b781d048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6996 Train acc 44.7939 Val loss 0.699 Val acc 44.6541\n",
      "Train loss 0.6975 Train acc 44.7939 Val loss 0.697 Val acc 44.6541\n",
      "Train loss 0.6955 Train acc 44.7939 Val loss 0.6951 Val acc 44.6541\n",
      "Train loss 0.6935 Train acc 44.8637 Val loss 0.6931 Val acc 45.283\n",
      "Train loss 0.6914 Train acc 49.3361 Val loss 0.6911 Val acc 49.6855\n",
      "Train loss 0.6891 Train acc 62.6136 Val loss 0.6888 Val acc 60.3774\n",
      "Train loss 0.6862 Train acc 69.9511 Val loss 0.6861 Val acc 71.0692\n",
      "Train loss 0.6826 Train acc 66.9462 Val loss 0.6828 Val acc 69.1824\n",
      "Train loss 0.6781 Train acc 65.5486 Val loss 0.6786 Val acc 64.7799\n",
      "Train loss 0.6724 Train acc 64.7799 Val loss 0.6734 Val acc 63.522\n",
      "Train loss 0.6654 Train acc 64.2907 Val loss 0.667 Val acc 63.522\n",
      "Train loss 0.6568 Train acc 64.6401 Val loss 0.6595 Val acc 62.8931\n",
      "Train loss 0.6463 Train acc 65.5486 Val loss 0.6507 Val acc 62.2642\n",
      "Train loss 0.6338 Train acc 67.0161 Val loss 0.6405 Val acc 65.4088\n",
      "Train loss 0.619 Train acc 69.1824 Val loss 0.6288 Val acc 66.0377\n",
      "Train loss 0.6017 Train acc 70.6499 Val loss 0.6157 Val acc 67.2956\n",
      "Train loss 0.582 Train acc 73.3054 Val loss 0.6014 Val acc 69.8113\n",
      "Train loss 0.56 Train acc 75.7512 Val loss 0.5862 Val acc 70.4403\n",
      "Train loss 0.5362 Train acc 78.4067 Val loss 0.5706 Val acc 74.2138\n",
      "Train loss 0.5112 Train acc 79.5248 Val loss 0.5556 Val acc 76.1006\n"
     ]
    }
   ],
   "source": [
    "best_val = 0\n",
    "for epoch_index in range(100):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    # Iterate over training dataset\n",
    "    # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size=len(dataset), device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # the training routine is 5 steps:\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # step 2. compute the output\n",
    "        y_pred = classifier(batch_dict['x_data'].float())\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch-running_loss) / (batch_index + 1)\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, batch_size=len(dataset), device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # step 1. compute the output\n",
    "        y_pred = classifier(batch_dict['x_data'].float())\n",
    "        # step 2. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        # step 3. compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    if(best_val<=running_acc):\n",
    "        torch.save({\n",
    "            'epoch': epoch_index,\n",
    "            'model_state_dict': classifier.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss,\n",
    "            }, \"OHE.h5\")\n",
    "        best_val = running_acc\n",
    "    \n",
    "    if epoch_index%5 == 0:\n",
    "        print('Train loss', round(train_state['train_loss'][-1], 4), 'Train acc', round(train_state['train_acc'][-1], 4), 'Val loss', round(train_state['val_loss'][-1], 4), 'Val acc', round(train_state['val_acc'][-1], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47e426c5-f7ac-42b4-81b9-1043a8b04d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=len(dataset),device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "checkpoint = torch.load(\"OHE.h5\")\n",
    "classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "classifier.eval()\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x=batch_dict['x_data'].float())\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "    # compute the accuracy\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a58987a1-242b-4099-8ca0-bbeec58e7d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.554\n",
      "Test Accuracy: 74.12\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd16e6d-5631-4084-9c0e-07e5029b2fd6",
   "metadata": {},
   "source": [
    "## Word embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fabab9-6b8e-4a62-b2fb-c1027b75e196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7197d34-4ece-47a0-92c7-c70aa5a363b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b541e-e0d1-47e1-9cb0-e9e5da6368ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d39e2-06a2-46f2-b34e-8eeb8cce1437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c81dd94f-25a3-41ee-b358-0c18ad12bda8",
   "metadata": {},
   "source": [
    "# Multi-class Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8441f32-e7f0-4a99-bf70-eccc3d6995f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb41a4ff-ec2c-450f-b17e-8399ad028617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7b93c56-8335-45c4-baad-7a15167fe595",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b34cae-0b1d-47df-b869-a8cc425a826b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5897a0-e467-41f9-ac90-32b633cdd6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4ebee4a-b42f-45f8-8597-4564290a0255",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9105157-14d8-42f7-8722-612cab5abbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f6d0eb7-94e0-4c23-9560-a2b71feff7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df['NARRATIVE'].str.len() < 30)\n",
    "df.loc[mask]['NARRATIVE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749c56c-8001-4982-8171-06f731be0fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
