{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ecf86b-1bd8-41c4-aaaf-e69f822df5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "import time \n",
    "import json\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing as mp\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from argparse import Namespace\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import codecs\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"): \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "        \n",
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\", mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\", end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token, 'mask_token': self._mask_token, 'begin_seq_token': self._begin_seq_token, 'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "class NERVectorizer(object):\n",
    "    def __init__(self, NARRATIVE_vocab, target_vocab):\n",
    "        self.NARRATIVE_vocab = NARRATIVE_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def vectorize(self, NARRATIVE, target, vector_length=-1, target_vector_length=-1):\n",
    "        indices = [self.NARRATIVE_vocab.begin_seq_index]\n",
    "        indices.extend(self.NARRATIVE_vocab.lookup_token(token) for token in NARRATIVE.split(\",\"))\n",
    "        indices.append(self.NARRATIVE_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        from_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        from_vector[vector_length - len(indices):] = indices\n",
    "        from_vector[:vector_length - len(indices)] = self.NARRATIVE_vocab.mask_index\n",
    "        \n",
    "        out_indices = [self.target_vocab.begin_seq_index]\n",
    "        out_indices.extend(self.target_vocab.lookup_token(token) for token in target.split(\",\"))\n",
    "        out_indices.append(self.target_vocab.end_seq_index)\n",
    "        \n",
    "        if target_vector_length < 0:\n",
    "            target_vector_length = len(out_indices)\n",
    "        \n",
    "        to_vector = np.zeros(target_vector_length, dtype=np.int64)\n",
    "        to_vector[target_vector_length - len(out_indices):] = out_indices\n",
    "        to_vector[:target_vector_length - len(out_indices)] = self.target_vocab.mask_index\n",
    "        return from_vector, to_vector, vector_length, target_vector_length\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, targets):\n",
    "        target_vocab = SequenceVocabulary()        \n",
    "        for target in sorted(targets):\n",
    "            target_vocab.add_token(target)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for NARRATIVE in df.NARRATIVE:\n",
    "            for token in NARRATIVE.split(\",\"):\n",
    "                word_counts[token] += 1\n",
    "\n",
    "        NARRATIVE_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            NARRATIVE_vocab.add_token(word)\n",
    "        return cls(NARRATIVE_vocab, target_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_vocab = SequenceVocabulary.from_serializable(contents['NARRATIVE_vocab'])\n",
    "        category_vocab = Vocabulary.from_serializable(contents['target_vocab'])\n",
    "        return cls(NARRATIVE_vocab=NARRATIVE_vocab, target_vocab=target_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'NARRATIVE_vocab': self.NARRATIVE_vocab.to_serializable(), 'target_vocab': self.target_vocab.to_serializable()}\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer, labels):\n",
    "        self.df = df\n",
    "        self._vectorizer = vectorizer\n",
    "        self.labels=labels\n",
    "\n",
    "        measure_len = lambda context: len(context.split(\",\"))\n",
    "        self._max_seq_length = max(map(measure_len, df.NARRATIVE)) + 2\n",
    "        \n",
    "        self._max_tag_length = max(len(labels), self._max_seq_length)\n",
    "        \n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size), 'val': (self.val_df, self.validation_size), 'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, news_csv, label_txt):\n",
    "        labels=[]\n",
    "        with codecs.open('labels.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                if len(line)>0:\n",
    "                    labels.append(line.strip())\n",
    "        \n",
    "        df = pd.read_csv(news_csv)\n",
    "        return cls(df, NERVectorizer.from_dataframe(df, labels), labels)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n",
    "        df = pd.read_csv(news_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(news_csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        NARRATIVE_vector, to_vector, length1, length2 = self._vectorizer.vectorize(row.NARRATIVE, row.target, self._max_seq_length, self._max_seq_length)\n",
    "        return {'x_data': NARRATIVE_vector,'y_target': to_vector, 'x_length': length1, 'y_length': length2}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc8938c-cb3d-4f38-a760-ca80c815eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.outputs = None\n",
    "        \n",
    "    def forward(self, src, tag, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        batch_size = tag.shape[1]\n",
    "        tag_len = tag.shape[0]\n",
    "        tag_vocab_size = self.decoder.output_dim\n",
    "        self.outputs = torch.zeros(tag_len, batch_size, tag_vocab_size).to(device)\n",
    "        \n",
    "        hidden, cell = self.encoder(src)\n",
    "        x = tag[0,:]\n",
    "        \n",
    "        for t in range(1, tag_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            self.outputs[t] = output\n",
    "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1) \n",
    "            x = tag[t] if teacher_force else top1\n",
    "        \n",
    "        return self.outputs\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=False)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        out = self.fc1(output.squeeze(0))\n",
    "\n",
    "        \n",
    "        return out, hidden, cell\n",
    "    \n",
    "def normalize_sizes(y_pred, y_true):\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f83007-7894-4ada-bd26-3d83954153e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NERDataset.load_dataset_and_make_vectorizer('data/ner.csv', 'label_txt')\n",
    "dataset.save_vectorizer('nervectorizer.json')\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a42c8cc-03ba-4ba3-8f8a-a536c1328b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "INPUT_DIM = len(vectorizer.NARRATIVE_vocab)\n",
    "OUTPUT_DIM = len(vectorizer.target_vocab)\n",
    "ENC_EMB_DIM = 50\n",
    "DEC_EMB_DIM = 50\n",
    "HID_DIM = 64\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df9c676-e2f4-4ddb-ad4d-e09648fd6643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(11965, 50)\n",
       "    (rnn): LSTM(50, 64)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(180, 50)\n",
       "    (rnn): LSTM(50, 64)\n",
       "    (fc1): Linear(in_features=64, out_features=180, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07583815-abff-4773-ab3f-f5dc6297a6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 4.384732127189636 Train acc 70.2216486235525\n",
      "Val loss 3.3224072456359863 Val acc 89.26405461058926\n",
      "Train loss 2.2976983189582825 Train acc 89.25764827421223\n",
      "Val loss 1.3219678401947021 Val acc 89.27533779018928\n",
      "Train loss 1.0050078417573656 Train acc 88.74998061146235\n",
      "Val loss 0.8085257411003113 Val acc 89.34867845758934\n",
      "Train loss 0.7824036266122545 Train acc 89.1278029759152\n",
      "Val loss 0.7507113218307495 Val acc 89.36278243208936\n",
      "Train loss 0.7406699785164425 Train acc 89.26284904259043\n",
      "Val loss 0.7258737087249756 Val acc 89.35996163718936\n",
      "Train loss 0.7210081986018588 Train acc 89.27608729560356\n",
      "Val loss 0.7173876166343689 Val acc 89.26123381568925\n",
      "Train loss 0.711839484316962 Train acc 89.28235719834866\n",
      "Val loss 0.7157663702964783 Val acc 89.2019971227892\n",
      "Train loss 0.7079586684703827 Train acc 89.26163781412859\n",
      "Val loss 0.7100784778594971 Val acc 89.21892189218921\n",
      "Train loss 0.7042489988463266 Train acc 89.28004704245849\n",
      "Val loss 0.7053143978118896 Val acc 89.26969620038928\n",
      "Train loss 0.7019621559551784 Train acc 89.28013105094949\n",
      "Val loss 0.7055802941322327 Val acc 89.22738427688924\n"
     ]
    }
   ],
   "source": [
    "for epoch_index in range(10):\n",
    "    dataset.set_split('train')\n",
    "    \n",
    "    batch_generator = generate_batches(dataset, batch_size=512, device=\"cuda\")\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_dict['x_data'].float(), batch_dict['y_target'].float())\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_target'], vectorizer.target_vocab.mask_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], vectorizer.target_vocab.mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "    \n",
    "    print(\"Train loss\", running_loss, \"Train acc\", running_acc)\n",
    "    \n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, batch_size=len(dataset), device=\"cuda\")\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    model.eval()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        \n",
    "        y_pred = model(batch_dict['x_data'].float(), batch_dict['y_target'].float())\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_target'], vectorizer.target_vocab.mask_index)\n",
    "        \n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], vectorizer.target_vocab.mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "    \n",
    "    print(\"Val loss\", running_loss, \"Val acc\", running_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43cc842c-33fe-488a-8223-78283590632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=len(dataset), device=\"cuda\")\n",
    "\n",
    "running_acc = 0.\n",
    "running_loss=0.\n",
    "model.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = model(batch_dict['x_data'].long(), batch_dict['y_target'].long())\n",
    "\n",
    "    # compute the loss\n",
    "    loss = sequence_loss(y_pred, batch_dict['y_target'], vectorizer.target_vocab.mask_index)\n",
    "    \n",
    "    # compute the running loss and running accuracy\n",
    "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'], vectorizer.target_vocab.mask_index)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "    \n",
    "    y_true=batch_dict['y_target'].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6cf28fa-892e-4f8f-8d53-8ab70ff64b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,  ..., 134, 179,   3],\n",
       "        [  0,   0,   0,  ..., 179, 179,   3],\n",
       "        [  0,   0,   0,  ..., 179, 179,   3],\n",
       "        ...,\n",
       "        [  0,   0,   0,  ..., 135, 179,   3],\n",
       "        [  0,   0,   0,  ..., 179, 179,   3],\n",
       "        [  0,   0,   0,  ..., 179, 179,   3]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08c6e705-3112-4d3c-bef9-730f6a143706",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = normalize_sizes(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f801d6a9-1810-4d20-b510-b4262fef07de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0,   0,  ..., 179, 179,   3], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, y_pred_indices = y_pred.max(dim=1)\n",
    "y_pred_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9167b317-cabb-4796-aea1-e4ac149f59ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0,   0,  ..., 179, 179,   3], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6f6c1-9bfd-44ed-926a-f38ac9cdac57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8761d46-c790-4c33-b083-5733b3eba272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811894e0-116a-4dac-be01-5d1e430e2ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa952801-2e20-420d-8a96-f18ee60d2d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f834d-51fe-40f3-9ea9-42a8ad0df79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a47ae-e939-41d8-ab1d-23a46b2c0d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368d93b-8c6e-4486-be71-e0978148465b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
